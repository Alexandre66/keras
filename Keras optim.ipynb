{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import the librairies that we will use later\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(12345)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We import 2 .csv as dataframe\n",
    "\n",
    "train_df = pd.read_csv('../data/fashion-mnist_train.csv')\n",
    "test_df = pd.read_csv('../data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0        30        43   \n",
       "3       0    ...            3         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         1         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create np.array because tensorflow use those\n",
    "\n",
    "train_data = np.array(train_df, dtype ='float32')\n",
    "test_data = np.array(test_df, dtype ='float32')\n",
    "\n",
    "# Then we split (and rescale) our data sets\n",
    "\n",
    "x_train = train_data[:, 1:] / 255   \n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "x_test = test_data[:, 1:] / 255\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create here a data set to train and one to validate\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=12345 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24896f8c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[500, :].reshape(28,28)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Create the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "im_rows =28\n",
    "im_cols = 28\n",
    "batch_size = 512\n",
    "im_shape = (im_rows,im_cols, 1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], *im_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *im_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *im_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (48000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "x_validate shape: (12000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('x_validate shape: {}'.format(x_validate.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size= (3,3), activation='relu', input_shape=im_shape),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss ='sparse_categorical_crossentropy',\n",
    "    optimizer = Adam(lr=0.001),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 0.2898 - acc: 0.8980 - val_loss: 0.2891 - val_acc: 0.8993\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 21s 432us/step - loss: 0.2822 - acc: 0.9013 - val_loss: 0.2817 - val_acc: 0.9014\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 19s 395us/step - loss: 0.2726 - acc: 0.9026 - val_loss: 0.2793 - val_acc: 0.9008\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 19s 391us/step - loss: 0.2643 - acc: 0.9069 - val_loss: 0.2794 - val_acc: 0.9021\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 20s 420us/step - loss: 0.2572 - acc: 0.9088 - val_loss: 0.2775 - val_acc: 0.9017\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 20s 409us/step - loss: 0.2522 - acc: 0.9105 - val_loss: 0.2713 - val_acc: 0.9026\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 20s 424us/step - loss: 0.2472 - acc: 0.9120 - val_loss: 0.2687 - val_acc: 0.9038\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 21s 429us/step - loss: 0.2435 - acc: 0.9127 - val_loss: 0.2615 - val_acc: 0.9048\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 22s 466us/step - loss: 0.2358 - acc: 0.9163 - val_loss: 0.2665 - val_acc: 0.9045\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 25s 522us/step - loss: 0.2292 - acc: 0.9181 - val_loss: 0.2589 - val_acc: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248b0bd5080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "cnn_model.fit(\n",
    "    x_train, y_train, batch_size=batch_size,\n",
    "    epochs =10, verbose= 1,\n",
    "    validation_data=(x_validate,y_validate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2542\n",
      " test acc: 0.9102\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_test,y_test,verbose=0)\n",
    "\n",
    "print('test loss: {:.4f}'.format(score[0]))\n",
    "\n",
    "print(' test acc: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III : Hyperopt of keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fashion_mnist():\n",
    "    from keras.datasets import fashion_mnist  # this requires keras>=2.0.9\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x = np.concatenate((x_train, x_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = np.divide(x, 255.)\n",
    "    print('Fashion MNIST samples', x.shape)\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    trainTotal = np.array(pd.read_csv('../data/fashion-mnist_train.csv'))\n",
    "    testTotal = np.array(pd.read_csv('../data/fashion-mnist_test.csv'))\n",
    "    (x_train, y_train) = trainTotal[:, 1:], trainTotal[:, 0]\n",
    "    (x_test, y_test) = testTotal[:, 1:], testTotal[:, 0]\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    nb_classes = 10\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]]),\n",
       " array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.00392157,\n",
       "         0.        ],\n",
       "        [0.        , 0.00392157, 0.01176471, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]], dtype=float32),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    # If we choose 'four', add an additional fourth layer\n",
    "    if conditional({{choice(['three', 'four'])}}) == 'four':\n",
    "        model.add(Dense(100))\n",
    "\n",
    "        # We can also choose between complete sets of layers\n",
    "\n",
    "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([64, 128])}},\n",
    "              epochs=5,\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.image as mpimg\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import fashion_mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']),\n",
      "        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: trainTotal = np.array(pd.read_csv('../data/fashion-mnist_train.csv'))\n",
      "  9: testTotal = np.array(pd.read_csv('../data/fashion-mnist_test.csv'))\n",
      " 10: (x_train, y_train) = trainTotal[:, 1:], trainTotal[:, 0]\n",
      " 11: (x_test, y_test) = testTotal[:, 1:], testTotal[:, 0]\n",
      " 12: x_train = x_train.reshape(60000, 784)\n",
      " 13: x_test = x_test.reshape(10000, 784)\n",
      " 14: x_train = x_train.astype('float32')\n",
      " 15: x_test = x_test.astype('float32')\n",
      " 16: x_train /= 255\n",
      " 17: x_test /= 255\n",
      " 18: nb_classes = 10\n",
      " 19: y_train = np_utils.to_categorical(y_train, nb_classes)\n",
      " 20: y_test = np_utils.to_categorical(y_test, nb_classes)\n",
      " 21: \n",
      " 22: \n",
      " 23: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14:     model.add(Dense(512, input_shape=(784,)))\n",
      "  15:     model.add(Activation('relu'))\n",
      "  16:     model.add(Dropout(space['Dropout']))\n",
      "  17:     model.add(Dense(space['Dense']))\n",
      "  18:     model.add(Activation(space['Activation']))\n",
      "  19:     model.add(Dropout(space['Dropout_1']))\n",
      "  20: \n",
      "  21:     # If we choose 'four', add an additional fourth layer\n",
      "  22:     if conditional(space['conditional']) == 'four':\n",
      "  23:         model.add(Dense(100))\n",
      "  24: \n",
      "  25:         # We can also choose between complete sets of layers\n",
      "  26: \n",
      "  27:         model.add(space['add'])\n",
      "  28:         model.add(Activation('relu'))\n",
      "  29: \n",
      "  30:     model.add(Dense(10))\n",
      "  31:     model.add(Activation('softmax'))\n",
      "  32: \n",
      "  33:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      "  34:                   optimizer=space['optimizer'])\n",
      "  35: \n",
      "  36:     model.fit(x_train, y_train,\n",
      "  37:               batch_size=space['batch_size'],\n",
      "  38:               epochs=5,\n",
      "  39:               verbose=2,\n",
      "  40:               validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 22s - loss: 1.7988 - acc: 0.3696 - val_loss: 0.7679 - val_acc: 0.7945\n",
      "Test accuracy: 0.7945\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 15s - loss: 2.2092 - acc: 0.2909 - val_loss: 0.7434 - val_acc: 0.7594\n",
      "Test accuracy: 0.7594\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 17s - loss: 1.9368 - acc: 0.3143 - val_loss: 0.6961 - val_acc: 0.8750\n",
      "Test accuracy: 0.875\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 12s - loss: 0.7112 - acc: 0.7721 - val_loss: 0.1989 - val_acc: 0.9405\n",
      "Test accuracy: 0.9405\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 29s - loss: 0.2833 - acc: 0.9141 - val_loss: 0.1336 - val_acc: 0.9574\n",
      "Test accuracy: 0.9574\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 2s 154us/step\n",
      "[6.700230030822754, 0.0642]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Activation': 1, 'Dense': 2, 'Dropout': 0.03323327852409652, 'Dropout_1': 0.0886198698550964, 'add': 1, 'batch_size': 0, 'conditional': 1, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Keras optim')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 193us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.324588710784912, 0.073]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.image as mpimg\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import fashion_mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']),\n",
      "        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: trainTotal = np.array(pd.read_csv('../data/fashion-mnist_train.csv'))\n",
      "  9: testTotal = np.array(pd.read_csv('../data/fashion-mnist_test.csv'))\n",
      " 10: (x_train, y_train) = trainTotal[:, 1:], trainTotal[:, 0]\n",
      " 11: (x_test, y_test) = testTotal[:, 1:], testTotal[:, 0]\n",
      " 12: x_train = x_train.reshape(60000, 784)\n",
      " 13: x_test = x_test.reshape(10000, 784)\n",
      " 14: x_train = x_train.astype('float32')\n",
      " 15: x_test = x_test.astype('float32')\n",
      " 16: x_train /= 255\n",
      " 17: x_test /= 255\n",
      " 18: nb_classes = 10\n",
      " 19: y_train = np_utils.to_categorical(y_train, nb_classes)\n",
      " 20: y_test = np_utils.to_categorical(y_test, nb_classes)\n",
      " 21: \n",
      " 22: \n",
      " 23: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14:     model.add(Dense(512, input_shape=(784,)))\n",
      "  15:     model.add(Activation('relu'))\n",
      "  16:     model.add(Dropout(space['Dropout']))\n",
      "  17:     model.add(Dense(space['Dense']))\n",
      "  18:     model.add(Activation(space['Activation']))\n",
      "  19:     model.add(Dropout(space['Dropout_1']))\n",
      "  20: \n",
      "  21:     # If we choose 'four', add an additional fourth layer\n",
      "  22:     if conditional(space['conditional']) == 'four':\n",
      "  23:         model.add(Dense(100))\n",
      "  24: \n",
      "  25:         # We can also choose between complete sets of layers\n",
      "  26: \n",
      "  27:         model.add(space['add'])\n",
      "  28:         model.add(Activation('relu'))\n",
      "  29: \n",
      "  30:     model.add(Dense(10))\n",
      "  31:     model.add(Activation('softmax'))\n",
      "  32: \n",
      "  33:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      "  34:                   optimizer=space['optimizer'])\n",
      "  35: \n",
      "  36:     model.fit(x_train, y_train,\n",
      "  37:               batch_size=space['batch_size'],\n",
      "  38:               epochs=5,\n",
      "  39:               verbose=2,\n",
      "  40:               validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 16s - loss: 1.7025 - acc: 0.3968 - val_loss: 0.7113 - val_acc: 0.8166\n",
      "Test accuracy: 0.8166\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 13s - loss: 2.2349 - acc: 0.2812 - val_loss: 0.7518 - val_acc: 0.7635\n",
      "Test accuracy: 0.7635\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 14s - loss: 1.9737 - acc: 0.3012 - val_loss: 0.7047 - val_acc: 0.8619\n",
      "Test accuracy: 0.8619\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 8s - loss: 0.7308 - acc: 0.7666 - val_loss: 0.2047 - val_acc: 0.9389\n",
      "Test accuracy: 0.9389\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 23s - loss: 0.2852 - acc: 0.9123 - val_loss: 0.1274 - val_acc: 0.9610\n",
      "Test accuracy: 0.961\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 12s - loss: 2.4381 - acc: 0.1050 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Test accuracy: 0.1135\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 18s - loss: 0.3647 - acc: 0.8927 - val_loss: 0.1398 - val_acc: 0.9618\n",
      "Test accuracy: 0.9618\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 12s - loss: 2.2437 - acc: 0.1756 - val_loss: 2.0296 - val_acc: 0.4298\n",
      "Test accuracy: 0.4298\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 0.4486 - acc: 0.8572 - val_loss: 0.1812 - val_acc: 0.9443\n",
      "Test accuracy: 0.9443\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 15s - loss: 0.3248 - acc: 0.9002 - val_loss: 0.1374 - val_acc: 0.9583\n",
      "Test accuracy: 0.9583\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 1s 110us/step\n",
      "[7.939725932312012, 0.0967]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Activation': 0, 'Dense': 1, 'Dropout': 0.2283813112902432, 'Dropout_1': 0.5422412690636627, 'add': 0, 'batch_size': 0, 'conditional': 1, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=rand.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Keras optim')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "?optim.minimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
