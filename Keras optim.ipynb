{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Here we import the librairies that we will use later\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(12345)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We import 2 .csv as dataframe\n",
    "\n",
    "train_df = pd.read_csv('../data/fashion-mnist_train.csv')\n",
    "test_df = pd.read_csv('../data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0        30        43   \n",
       "3       0    ...            3         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         1         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create np.array because tensorflow use those\n",
    "\n",
    "train_data = np.array(train_df, dtype ='float32')\n",
    "test_data = np.array(test_df, dtype ='float32')\n",
    "\n",
    "# Then we split (and rescale) our data sets\n",
    "\n",
    "x_train = train_data[:, 1:] / 255   \n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "x_test = test_data[:, 1:] / 255\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create here a data set to train and one to validate\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=12345 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2b5fcdef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[500, :].reshape(28,28)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Create the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "im_rows =28\n",
    "im_cols = 28\n",
    "batch_size = 512\n",
    "im_shape = (im_rows,im_cols, 1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], *im_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *im_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *im_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (48000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "x_validate shape: (12000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('x_validate shape: {}'.format(x_validate.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size= (3,3), activation='relu', input_shape=im_shape),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss ='sparse_categorical_crossentropy',\n",
    "    optimizer = Adam(lr=0.001),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 16s - loss: 0.2827 - acc: 0.9001 - val_loss: 0.2851 - val_acc: 0.9026\n",
      "Epoch 2/10\n",
      " - 17s - loss: 0.2750 - acc: 0.9015 - val_loss: 0.2806 - val_acc: 0.9018\n",
      "Epoch 3/10\n",
      " - 16s - loss: 0.2693 - acc: 0.9036 - val_loss: 0.2724 - val_acc: 0.9055\n",
      "Epoch 4/10\n",
      " - 16s - loss: 0.2606 - acc: 0.9072 - val_loss: 0.2673 - val_acc: 0.9078\n",
      "Epoch 5/10\n",
      " - 16s - loss: 0.2556 - acc: 0.9102 - val_loss: 0.2683 - val_acc: 0.9059\n",
      "Epoch 6/10\n",
      " - 16s - loss: 0.2503 - acc: 0.9106 - val_loss: 0.2653 - val_acc: 0.9080\n",
      "Epoch 7/10\n",
      " - 17s - loss: 0.2457 - acc: 0.9116 - val_loss: 0.2607 - val_acc: 0.9071\n",
      "Epoch 8/10\n",
      " - 17s - loss: 0.2441 - acc: 0.9120 - val_loss: 0.2651 - val_acc: 0.9076\n",
      "Epoch 9/10\n",
      " - 18s - loss: 0.2362 - acc: 0.9152 - val_loss: 0.2554 - val_acc: 0.9125\n",
      "Epoch 10/10\n",
      " - 18s - loss: 0.2341 - acc: 0.9158 - val_loss: 0.2560 - val_acc: 0.9109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d303ab50f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "cnn_model.fit(\n",
    "    x_train, y_train, batch_size=batch_size,\n",
    "    epochs =10, verbose= 2,\n",
    "    validation_data=(x_test,y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2986\n",
      " test acc: 0.8959\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_test,y_test,verbose=0)\n",
    "\n",
    "print('test loss: {:.4f}'.format(score[0]))\n",
    "\n",
    "print(' test acc: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III : Hyperopt of keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fashion_mnist():\n",
    "    from keras.datasets import fashion_mnist  # this requires keras>=2.0.9\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x = np.concatenate((x_train, x_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = np.divide(x, 255.)\n",
    "    print('Fashion MNIST samples', x.shape)\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    trainTotal = np.array(pd.read_csv('../data/fashion-mnist_train.csv'))\n",
    "    testTotal = np.array(pd.read_csv('../data/fashion-mnist_test.csv'))\n",
    "    (x_train, y_train) = trainTotal[:, 1:], trainTotal[:, 0]\n",
    "    (x_test, y_test) = testTotal[:, 1:], testTotal[:, 0]\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    nb_classes = 10\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]]),\n",
       " array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.00392157,\n",
       "         0.        ],\n",
       "        [0.        , 0.00392157, 0.01176471, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]], dtype=float32),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    # If we choose 'four', add an additional fourth layer\n",
    "    if conditional({{choice(['three', 'four'])}}) == 'four':\n",
    "        model.add(Dense(100))\n",
    "\n",
    "        # We can also choose between complete sets of layers\n",
    "\n",
    "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([64, 128])}},\n",
    "              epochs=20,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.image as mpimg\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import fashion_mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']),\n",
      "        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: trainTotal = np.array(pd.read_csv('../data/fashion-mnist_train.csv'))\n",
      "  9: testTotal = np.array(pd.read_csv('../data/fashion-mnist_test.csv'))\n",
      " 10: (x_train, y_train) = trainTotal[:, 1:], trainTotal[:, 0]\n",
      " 11: (x_test, y_test) = testTotal[:, 1:], testTotal[:, 0]\n",
      " 12: x_train = x_train.reshape(60000, 784)\n",
      " 13: x_test = x_test.reshape(10000, 784)\n",
      " 14: x_train = x_train.astype('float32')\n",
      " 15: x_test = x_test.astype('float32')\n",
      " 16: x_train /= 255\n",
      " 17: x_test /= 255\n",
      " 18: nb_classes = 10\n",
      " 19: y_train = np_utils.to_categorical(y_train, nb_classes)\n",
      " 20: y_test = np_utils.to_categorical(y_test, nb_classes)\n",
      " 21: \n",
      " 22: \n",
      " 23: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14:     model.add(Dense(512, input_shape=(784,)))\n",
      "  15:     model.add(Activation('relu'))\n",
      "  16:     model.add(Dropout(space['Dropout']))\n",
      "  17:     model.add(Dense(space['Dense']))\n",
      "  18:     model.add(Activation(space['Activation']))\n",
      "  19:     model.add(Dropout(space['Dropout_1']))\n",
      "  20: \n",
      "  21:     # If we choose 'four', add an additional fourth layer\n",
      "  22:     if conditional(space['conditional']) == 'four':\n",
      "  23:         model.add(Dense(100))\n",
      "  24: \n",
      "  25:         # We can also choose between complete sets of layers\n",
      "  26: \n",
      "  27:         model.add(space['add'])\n",
      "  28:         model.add(Activation('relu'))\n",
      "  29: \n",
      "  30:     model.add(Dense(10))\n",
      "  31:     model.add(Activation('softmax'))\n",
      "  32: \n",
      "  33:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      "  34:                   optimizer=space['optimizer'])\n",
      "  35: \n",
      "  36:     model.fit(x_train, y_train,\n",
      "  37:               batch_size=space['batch_size'],\n",
      "  38:               epochs=20,\n",
      "  39:               verbose=1,\n",
      "  40:               validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 1.5767 - acc: 0.4117 - val_loss: 0.8449 - val_acc: 0.6548\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.9747 - acc: 0.6244 - val_loss: 0.7160 - val_acc: 0.7191\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.8334 - acc: 0.6805 - val_loss: 0.6430 - val_acc: 0.7564\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.7574 - acc: 0.7160 - val_loss: 0.5860 - val_acc: 0.7767\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.6987 - acc: 0.7430 - val_loss: 0.5499 - val_acc: 0.8034\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.6587 - acc: 0.7621 - val_loss: 0.5144 - val_acc: 0.8141\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 16s 258us/step - loss: 0.6260 - acc: 0.7786 - val_loss: 0.4872 - val_acc: 0.8332\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.6008 - acc: 0.7880 - val_loss: 0.4653 - val_acc: 0.8380\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 0.5778 - acc: 0.7987 - val_loss: 0.4476 - val_acc: 0.8421\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 17s 282us/step - loss: 0.5579 - acc: 0.8048 - val_loss: 0.4449 - val_acc: 0.8460\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.5447 - acc: 0.8100 - val_loss: 0.4290 - val_acc: 0.8482\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 20s 329us/step - loss: 0.5306 - acc: 0.8155 - val_loss: 0.4187 - val_acc: 0.8507\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.5172 - acc: 0.8202 - val_loss: 0.4125 - val_acc: 0.8536\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 20s 325us/step - loss: 0.5106 - acc: 0.8241 - val_loss: 0.4106 - val_acc: 0.8566\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 17s 285us/step - loss: 0.4999 - acc: 0.8282 - val_loss: 0.3984 - val_acc: 0.8596\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.4913 - acc: 0.8301 - val_loss: 0.3967 - val_acc: 0.8600\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.4819 - acc: 0.8345 - val_loss: 0.3908 - val_acc: 0.8594\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 0.4773 - acc: 0.8350 - val_loss: 0.3844 - val_acc: 0.8651\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 19s 309us/step - loss: 0.4690 - acc: 0.8380 - val_loss: 0.3788 - val_acc: 0.8666\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.4666 - acc: 0.8403 - val_loss: 0.3760 - val_acc: 0.8682\n",
      "Test accuracy: 0.8682\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 2.2344 - acc: 0.2473 - val_loss: 1.0140 - val_acc: 0.6075\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 1.5660 - acc: 0.3735 - val_loss: 0.9188 - val_acc: 0.6159\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 1.4646 - acc: 0.4136 - val_loss: 0.8511 - val_acc: 0.6615\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 1.4168 - acc: 0.4375 - val_loss: 0.8136 - val_acc: 0.6766\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 1.3981 - acc: 0.4498 - val_loss: 0.7843 - val_acc: 0.6973\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 1.3826 - acc: 0.4582 - val_loss: 0.7787 - val_acc: 0.6766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 1.3564 - acc: 0.4711 - val_loss: 0.7765 - val_acc: 0.6615\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 1.3421 - acc: 0.4813 - val_loss: 0.7473 - val_acc: 0.7089\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 1.3396 - acc: 0.4843 - val_loss: 0.7244 - val_acc: 0.7113\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 1.3251 - acc: 0.4886 - val_loss: 0.6994 - val_acc: 0.7422\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 1.3155 - acc: 0.4966 - val_loss: 0.7027 - val_acc: 0.7292\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 1.3097 - acc: 0.5016 - val_loss: 0.7038 - val_acc: 0.7311\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 14s 242us/step - loss: 1.3077 - acc: 0.5034 - val_loss: 0.7025 - val_acc: 0.7145\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 1.3062 - acc: 0.5040 - val_loss: 0.6850 - val_acc: 0.7201\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 1.2887 - acc: 0.5109 - val_loss: 0.6794 - val_acc: 0.7304\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 1.3013 - acc: 0.5076 - val_loss: 0.6769 - val_acc: 0.7253\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 1.2997 - acc: 0.5101 - val_loss: 0.6911 - val_acc: 0.7184\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 1.2989 - acc: 0.5115 - val_loss: 0.6748 - val_acc: 0.7180\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 1.2990 - acc: 0.5111 - val_loss: 0.6746 - val_acc: 0.7252\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 1.2871 - acc: 0.5162 - val_loss: 0.6646 - val_acc: 0.7220\n",
      "Test accuracy: 0.722\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 1.9891 - acc: 0.2780 - val_loss: 1.0308 - val_acc: 0.7214\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 16s 264us/step - loss: 1.4273 - acc: 0.4516 - val_loss: 0.7745 - val_acc: 0.7228\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 1.2698 - acc: 0.5048 - val_loss: 0.6451 - val_acc: 0.7489\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 1.2018 - acc: 0.5244 - val_loss: 0.6248 - val_acc: 0.7906\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 1.1769 - acc: 0.5381 - val_loss: 0.5957 - val_acc: 0.7516\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 1.1445 - acc: 0.5540 - val_loss: 0.5877 - val_acc: 0.7956\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 1.1209 - acc: 0.5641 - val_loss: 0.5788 - val_acc: 0.8089\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.1118 - acc: 0.5693 - val_loss: 0.5632 - val_acc: 0.7940\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 1.0860 - acc: 0.5808 - val_loss: 0.5578 - val_acc: 0.8088\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 1.0787 - acc: 0.5846 - val_loss: 0.5448 - val_acc: 0.8219\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 1.0683 - acc: 0.5922 - val_loss: 0.5548 - val_acc: 0.8112\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 1.0505 - acc: 0.6004 - val_loss: 0.5417 - val_acc: 0.8191\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 1.0386 - acc: 0.6045 - val_loss: 0.5153 - val_acc: 0.8351\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 1.0363 - acc: 0.6083 - val_loss: 0.5165 - val_acc: 0.8414\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 1.0312 - acc: 0.6130 - val_loss: 0.5158 - val_acc: 0.8375\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 1.0210 - acc: 0.6161 - val_loss: 0.5221 - val_acc: 0.8369\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.0103 - acc: 0.6206 - val_loss: 0.4931 - val_acc: 0.8482\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 1.0077 - acc: 0.6230 - val_loss: 0.4908 - val_acc: 0.8471\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.9983 - acc: 0.6255 - val_loss: 0.5035 - val_acc: 0.8476\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.9881 - acc: 0.6297 - val_loss: 0.4985 - val_acc: 0.8402\n",
      "Test accuracy: 0.8402\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.8579 - acc: 0.6832 - val_loss: 0.4828 - val_acc: 0.8271\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.5408 - acc: 0.8097 - val_loss: 0.4156 - val_acc: 0.8509\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.4782 - acc: 0.8322 - val_loss: 0.4115 - val_acc: 0.8510\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.4413 - acc: 0.8440 - val_loss: 0.3843 - val_acc: 0.8596\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.4157 - acc: 0.8518 - val_loss: 0.3572 - val_acc: 0.8688\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.3952 - acc: 0.8603 - val_loss: 0.3467 - val_acc: 0.8751\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.3757 - acc: 0.8659 - val_loss: 0.3322 - val_acc: 0.8809\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.3625 - acc: 0.8698 - val_loss: 0.3266 - val_acc: 0.8800\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.3517 - acc: 0.8752 - val_loss: 0.3278 - val_acc: 0.8798\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3434 - acc: 0.8781 - val_loss: 0.3187 - val_acc: 0.8835\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.3335 - acc: 0.8794 - val_loss: 0.3116 - val_acc: 0.8859\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.3253 - acc: 0.8820 - val_loss: 0.3042 - val_acc: 0.8875\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.3158 - acc: 0.8858 - val_loss: 0.3042 - val_acc: 0.8891\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.3065 - acc: 0.8904 - val_loss: 0.3046 - val_acc: 0.8897\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.3000 - acc: 0.8903 - val_loss: 0.2990 - val_acc: 0.8897\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.2975 - acc: 0.8928 - val_loss: 0.3094 - val_acc: 0.8867\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.2895 - acc: 0.8956 - val_loss: 0.3048 - val_acc: 0.8891\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.2834 - acc: 0.8973 - val_loss: 0.3044 - val_acc: 0.8909\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.2800 - acc: 0.8978 - val_loss: 0.3013 - val_acc: 0.8880\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.2785 - acc: 0.8986 - val_loss: 0.2991 - val_acc: 0.8933\n",
      "Test accuracy: 0.8933\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 21s 358us/step - loss: 0.5498 - acc: 0.7977 - val_loss: 0.4642 - val_acc: 0.8296\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 0.3885 - acc: 0.8572 - val_loss: 0.3930 - val_acc: 0.8468\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 0.3592 - acc: 0.8701 - val_loss: 0.3597 - val_acc: 0.8679\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 21s 345us/step - loss: 0.3419 - acc: 0.8747 - val_loss: 0.3878 - val_acc: 0.8656\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 21s 358us/step - loss: 0.3334 - acc: 0.8805 - val_loss: 0.4690 - val_acc: 0.8231 0.3314 - acc: 0.8 - ETA: 1s - loss: 0.3\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 0.3272 - acc: 0.8830 - val_loss: 0.3573 - val_acc: 0.8710\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 0.3224 - acc: 0.8850 - val_loss: 0.3406 - val_acc: 0.8832\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 0.3183 - acc: 0.8862 - val_loss: 0.3906 - val_acc: 0.8638\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 22s 360us/step - loss: 0.3176 - acc: 0.8881 - val_loss: 0.3303 - val_acc: 0.8854\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 0.3165 - acc: 0.8900 - val_loss: 0.3469 - val_acc: 0.8806\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 21s 344us/step - loss: 0.3136 - acc: 0.8918 - val_loss: 0.3654 - val_acc: 0.8781\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 21s 357us/step - loss: 0.3074 - acc: 0.8924 - val_loss: 0.3474 - val_acc: 0.8847\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 22s 370us/step - loss: 0.3031 - acc: 0.8941 - val_loss: 0.4055 - val_acc: 0.8747\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 0.3027 - acc: 0.8946 - val_loss: 0.4597 - val_acc: 0.8758\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 23s 389us/step - loss: 0.3019 - acc: 0.8951 - val_loss: 0.3611 - val_acc: 0.8851\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.2977 - acc: 0.8984 - val_loss: 0.3941 - val_acc: 0.8862\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 0.2941 - acc: 0.8981 - val_loss: 0.3652 - val_acc: 0.8802\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.2898 - acc: 0.8994 - val_loss: 0.3736 - val_acc: 0.8919\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 22s 364us/step - loss: 0.2896 - acc: 0.9005 - val_loss: 0.3833 - val_acc: 0.8846\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 23s 381us/step - loss: 0.2840 - acc: 0.9026 - val_loss: 0.3402 - val_acc: 0.8907\n",
      "Test accuracy: 0.8907\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 2.4312 - acc: 0.1045 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 2.3120 - acc: 0.1100 - val_loss: 2.3028 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 2.2979 - acc: 0.1155 - val_loss: 2.2882 - val_acc: 0.1435\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 2.2760 - acc: 0.1269 - val_loss: 2.1716 - val_acc: 0.2871\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 2.2132 - acc: 0.1451 - val_loss: 1.9168 - val_acc: 0.2702\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 2.1029 - acc: 0.1688 - val_loss: 1.7411 - val_acc: 0.3007\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 1.9938 - acc: 0.1856 - val_loss: 1.6831 - val_acc: 0.2705\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 1.9179 - acc: 0.1968 - val_loss: 1.6649 - val_acc: 0.2284\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 1.8711 - acc: 0.1976 - val_loss: 1.6643 - val_acc: 0.3059\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 1.8365 - acc: 0.2077 - val_loss: 1.6601 - val_acc: 0.2052\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 1.8135 - acc: 0.2109 - val_loss: 1.6502 - val_acc: 0.2390\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 1.7934 - acc: 0.2140 - val_loss: 1.6614 - val_acc: 0.2304\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 1.7846 - acc: 0.2182 - val_loss: 1.6323 - val_acc: 0.2342\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 1.7672 - acc: 0.2242 - val_loss: 1.6242 - val_acc: 0.2178\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 1.7608 - acc: 0.2246 - val_loss: 1.6075 - val_acc: 0.3030\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 1.7423 - acc: 0.2364 - val_loss: 1.5772 - val_acc: 0.3262\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 1.7275 - acc: 0.2457 - val_loss: 1.5318 - val_acc: 0.3347\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 1.7111 - acc: 0.2564 - val_loss: 1.4839 - val_acc: 0.3257\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 1.6803 - acc: 0.2704 - val_loss: 1.4485 - val_acc: 0.3248\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 1.6580 - acc: 0.2814 - val_loss: 1.4354 - val_acc: 0.3399\n",
      "Test accuracy: 0.3399\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.6828 - acc: 0.7649 - val_loss: 0.4264 - val_acc: 0.8542\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 17s 275us/step - loss: 0.5138 - acc: 0.8294 - val_loss: 0.4076 - val_acc: 0.8646\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.5117 - acc: 0.8364 - val_loss: 0.4519 - val_acc: 0.8554\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.5123 - acc: 0.8401 - val_loss: 0.4214 - val_acc: 0.8600\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.5273 - acc: 0.8408 - val_loss: 0.4224 - val_acc: 0.8623\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 0.5291 - acc: 0.8430 - val_loss: 0.4314 - val_acc: 0.8651\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.5359 - acc: 0.8439 - val_loss: 0.4471 - val_acc: 0.8588\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.5368 - acc: 0.8466 - val_loss: 0.4688 - val_acc: 0.8599\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.5532 - acc: 0.8437 - val_loss: 0.4305 - val_acc: 0.8626\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.5562 - acc: 0.8449 - val_loss: 0.4780 - val_acc: 0.8620\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 16s 258us/step - loss: 0.5657 - acc: 0.8452 - val_loss: 0.4549 - val_acc: 0.8673\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.5712 - acc: 0.8441 - val_loss: 0.4862 - val_acc: 0.8617\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.5741 - acc: 0.8446 - val_loss: 0.5446 - val_acc: 0.8617\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 17s 282us/step - loss: 0.5762 - acc: 0.8451 - val_loss: 0.4775 - val_acc: 0.8605\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.5926 - acc: 0.8445 - val_loss: 0.5381 - val_acc: 0.8595\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.5956 - acc: 0.8429 - val_loss: 0.5130 - val_acc: 0.8565\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.6000 - acc: 0.8463 - val_loss: 0.5389 - val_acc: 0.8517\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 16s 264us/step - loss: 0.6153 - acc: 0.8456 - val_loss: 0.4854 - val_acc: 0.8720\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.6291 - acc: 0.8439 - val_loss: 0.5365 - val_acc: 0.8473\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.6361 - acc: 0.8443 - val_loss: 0.5208 - val_acc: 0.8680\n",
      "Test accuracy: 0.868\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 11s 184us/step - loss: 1.9359 - acc: 0.3064 - val_loss: 1.2570 - val_acc: 0.6503\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 1.2135 - acc: 0.5516 - val_loss: 0.8841 - val_acc: 0.6964\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.9796 - acc: 0.6309 - val_loss: 0.7671 - val_acc: 0.7250\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.8779 - acc: 0.6653 - val_loss: 0.7044 - val_acc: 0.7386\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.8161 - acc: 0.6897 - val_loss: 0.6731 - val_acc: 0.7462\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.7823 - acc: 0.7006 - val_loss: 0.6498 - val_acc: 0.7571\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.7553 - acc: 0.7127 - val_loss: 0.6345 - val_acc: 0.7636\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.7335 - acc: 0.7205 - val_loss: 0.6178 - val_acc: 0.7665\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.7174 - acc: 0.7275 - val_loss: 0.6036 - val_acc: 0.7707\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.6989 - acc: 0.7357 - val_loss: 0.5955 - val_acc: 0.7787\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.6830 - acc: 0.7416 - val_loss: 0.5804 - val_acc: 0.7806\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.6741 - acc: 0.7463 - val_loss: 0.5763 - val_acc: 0.7862\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.6610 - acc: 0.7524 - val_loss: 0.5615 - val_acc: 0.7889\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.6518 - acc: 0.7564 - val_loss: 0.5541 - val_acc: 0.7937\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.6444 - acc: 0.7589 - val_loss: 0.5594 - val_acc: 0.7906\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.6314 - acc: 0.7651 - val_loss: 0.5432 - val_acc: 0.7981\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.6278 - acc: 0.7671 - val_loss: 0.5340 - val_acc: 0.8021\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.6194 - acc: 0.7706 - val_loss: 0.5262 - val_acc: 0.8067\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.6122 - acc: 0.7718 - val_loss: 0.5214 - val_acc: 0.8076\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.6017 - acc: 0.7784 - val_loss: 0.5132 - val_acc: 0.8125\n",
      "Test accuracy: 0.8125\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.6802 - acc: 0.7467 - val_loss: 0.4773 - val_acc: 0.8210\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.4832 - acc: 0.8244 - val_loss: 0.4078 - val_acc: 0.8518\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.4413 - acc: 0.8392 - val_loss: 0.3752 - val_acc: 0.8612\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.4189 - acc: 0.8484 - val_loss: 0.3671 - val_acc: 0.8660\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.4056 - acc: 0.8552 - val_loss: 0.3647 - val_acc: 0.8657\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.3938 - acc: 0.8576 - val_loss: 0.3530 - val_acc: 0.8735\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3873 - acc: 0.8599 - val_loss: 0.3442 - val_acc: 0.8780\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3800 - acc: 0.8628 - val_loss: 0.3416 - val_acc: 0.8762\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.3711 - acc: 0.8660 - val_loss: 0.3484 - val_acc: 0.8753\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.3694 - acc: 0.8690 - val_loss: 0.3380 - val_acc: 0.8806\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3673 - acc: 0.8706 - val_loss: 0.3350 - val_acc: 0.8785\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3622 - acc: 0.8705 - val_loss: 0.3379 - val_acc: 0.8787\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3606 - acc: 0.8730 - val_loss: 0.3226 - val_acc: 0.8831\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.3563 - acc: 0.8735 - val_loss: 0.3346 - val_acc: 0.8812\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.3537 - acc: 0.8754 - val_loss: 0.3286 - val_acc: 0.8827\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.3512 - acc: 0.8762 - val_loss: 0.3545 - val_acc: 0.8765\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.3494 - acc: 0.8766 - val_loss: 0.3277 - val_acc: 0.8784\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.3488 - acc: 0.8774 - val_loss: 0.3312 - val_acc: 0.8819\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.3439 - acc: 0.8797 - val_loss: 0.3267 - val_acc: 0.8853\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.3431 - acc: 0.8790 - val_loss: 0.3186 - val_acc: 0.8884\n",
      "Test accuracy: 0.8884\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.5578 - acc: 0.7962 - val_loss: 0.4094 - val_acc: 0.8513\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 57s 951us/step - loss: 0.4147 - acc: 0.8487 - val_loss: 0.3641 - val_acc: 0.8676\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.3806 - acc: 0.8589 - val_loss: 0.3441 - val_acc: 0.8753\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.3536 - acc: 0.8693 - val_loss: 0.3319 - val_acc: 0.8762\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.3402 - acc: 0.8735 - val_loss: 0.3212 - val_acc: 0.8830\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.3224 - acc: 0.8801 - val_loss: 0.3387 - val_acc: 0.8793\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.3162 - acc: 0.8822 - val_loss: 0.3192 - val_acc: 0.8806\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.3016 - acc: 0.8880 - val_loss: 0.2958 - val_acc: 0.8892\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2947 - acc: 0.8893 - val_loss: 0.2938 - val_acc: 0.8938\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2882 - acc: 0.8928 - val_loss: 0.2969 - val_acc: 0.8914\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 0.2805 - acc: 0.8941 - val_loss: 0.2881 - val_acc: 0.8937\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2747 - acc: 0.8958 - val_loss: 0.2829 - val_acc: 0.8955\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.2679 - acc: 0.8982 - val_loss: 0.2915 - val_acc: 0.8942\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2608 - acc: 0.9011 - val_loss: 0.2968 - val_acc: 0.8909\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.2560 - acc: 0.9040 - val_loss: 0.2845 - val_acc: 0.8967\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2499 - acc: 0.9057 - val_loss: 0.2830 - val_acc: 0.8944\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2467 - acc: 0.9069 - val_loss: 0.2771 - val_acc: 0.8998\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2427 - acc: 0.9073 - val_loss: 0.2690 - val_acc: 0.9037\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2391 - acc: 0.9093 - val_loss: 0.2793 - val_acc: 0.8986\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2360 - acc: 0.9104 - val_loss: 0.2678 - val_acc: 0.9025\n",
      "Test accuracy: 0.9025\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 8s 774us/step\n",
      "[0.26776398167610166, 0.9025]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Activation': 1, 'Dense': 2, 'Dropout': 0.4143619965361732, 'Dropout_1': 0.09225974322037533, 'add': 1, 'batch_size': 1, 'conditional': 0, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Keras optim')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Activation': 1,\n",
       " 'Dense': 0,\n",
       " 'Dropout': 0.011725082462182734,\n",
       " 'Dropout_1': 0.46207837014862785,\n",
       " 'add': 1,\n",
       " 'batch_size': 1,\n",
       " 'conditional': 1,\n",
       " 'optimizer': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.image as mpimg\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import fashion_mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']),\n",
      "        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: trainTotal = np.array(pd.read_csv('../data/fashion-mnist_train.csv'))\n",
      "  9: testTotal = np.array(pd.read_csv('../data/fashion-mnist_test.csv'))\n",
      " 10: (x_train, y_train) = trainTotal[:, 1:], trainTotal[:, 0]\n",
      " 11: (x_test, y_test) = testTotal[:, 1:], testTotal[:, 0]\n",
      " 12: x_train = x_train.reshape(60000, 784)\n",
      " 13: x_test = x_test.reshape(10000, 784)\n",
      " 14: x_train = x_train.astype('float32')\n",
      " 15: x_test = x_test.astype('float32')\n",
      " 16: x_train /= 255\n",
      " 17: x_test /= 255\n",
      " 18: nb_classes = 10\n",
      " 19: y_train = np_utils.to_categorical(y_train, nb_classes)\n",
      " 20: y_test = np_utils.to_categorical(y_test, nb_classes)\n",
      " 21: \n",
      " 22: \n",
      " 23: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14:     model.add(Dense(512, input_shape=(784,)))\n",
      "  15:     model.add(Activation('relu'))\n",
      "  16:     model.add(Dropout(space['Dropout']))\n",
      "  17:     model.add(Dense(space['Dense']))\n",
      "  18:     model.add(Activation(space['Activation']))\n",
      "  19:     model.add(Dropout(space['Dropout_1']))\n",
      "  20: \n",
      "  21:     # If we choose 'four', add an additional fourth layer\n",
      "  22:     if conditional(space['conditional']) == 'four':\n",
      "  23:         model.add(Dense(100))\n",
      "  24: \n",
      "  25:         # We can also choose between complete sets of layers\n",
      "  26: \n",
      "  27:         model.add(space['add'])\n",
      "  28:         model.add(Activation('relu'))\n",
      "  29: \n",
      "  30:     model.add(Dense(10))\n",
      "  31:     model.add(Activation('softmax'))\n",
      "  32: \n",
      "  33:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      "  34:                   optimizer=space['optimizer'])\n",
      "  35: \n",
      "  36:     model.fit(x_train, y_train,\n",
      "  37:               batch_size=space['batch_size'],\n",
      "  38:               epochs=20,\n",
      "  39:               verbose=1,\n",
      "  40:               validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 20s 333us/step - loss: 1.5840 - acc: 0.4120 - val_loss: 0.8556 - val_acc: 0.6750\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.9743 - acc: 0.6260 - val_loss: 0.7052 - val_acc: 0.7373\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 18s 302us/step - loss: 0.8321 - acc: 0.6859 - val_loss: 0.6306 - val_acc: 0.7546\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.7486 - acc: 0.7250 - val_loss: 0.5831 - val_acc: 0.7870\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.6897 - acc: 0.7469 - val_loss: 0.5330 - val_acc: 0.8101\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.6497 - acc: 0.7650 - val_loss: 0.5067 - val_acc: 0.8199\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 18s 296us/step - loss: 0.6191 - acc: 0.7803 - val_loss: 0.4874 - val_acc: 0.8306\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.5950 - acc: 0.7895 - val_loss: 0.4672 - val_acc: 0.8333\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.5752 - acc: 0.7997 - val_loss: 0.4498 - val_acc: 0.8391\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.5595 - acc: 0.8040 - val_loss: 0.4372 - val_acc: 0.8429\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.5409 - acc: 0.8139 - val_loss: 0.4273 - val_acc: 0.8479\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 18s 303us/step - loss: 0.5274 - acc: 0.8168 - val_loss: 0.4187 - val_acc: 0.8494\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.5192 - acc: 0.8185 - val_loss: 0.4155 - val_acc: 0.8531\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.5066 - acc: 0.8248 - val_loss: 0.4044 - val_acc: 0.8571\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 21s 354us/step - loss: 0.5004 - acc: 0.8259 - val_loss: 0.4001 - val_acc: 0.8563\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.4894 - acc: 0.8307 - val_loss: 0.3936 - val_acc: 0.8596\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 21s 348us/step - loss: 0.4837 - acc: 0.8319 - val_loss: 0.3864 - val_acc: 0.8642\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.4755 - acc: 0.8369 - val_loss: 0.3820 - val_acc: 0.8644\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 0.4672 - acc: 0.8371 - val_loss: 0.3808 - val_acc: 0.8645\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.4623 - acc: 0.8386 - val_loss: 0.3763 - val_acc: 0.8657\n",
      "Test accuracy: 0.8657\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 2.2446 - acc: 0.2441 - val_loss: 1.0311 - val_acc: 0.5501\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 1.5705 - acc: 0.3693 - val_loss: 0.9079 - val_acc: 0.6177\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 1.4588 - acc: 0.4130 - val_loss: 0.8618 - val_acc: 0.6673\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 1.4195 - acc: 0.4323 - val_loss: 0.8490 - val_acc: 0.6532\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 1.3958 - acc: 0.4468 - val_loss: 0.7963 - val_acc: 0.6793\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 1.3608 - acc: 0.4663 - val_loss: 0.7617 - val_acc: 0.6946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 1.3572 - acc: 0.4701 - val_loss: 0.7431 - val_acc: 0.7085\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 1.3419 - acc: 0.4756 - val_loss: 0.7209 - val_acc: 0.7206\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 1.3275 - acc: 0.4879 - val_loss: 0.7324 - val_acc: 0.7073\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 1.3225 - acc: 0.4913 - val_loss: 0.7227 - val_acc: 0.7277\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 1.3060 - acc: 0.4989 - val_loss: 0.6990 - val_acc: 0.7154\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 16s 264us/step - loss: 1.3049 - acc: 0.5007 - val_loss: 0.6866 - val_acc: 0.7206\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 1.3079 - acc: 0.5009 - val_loss: 0.6947 - val_acc: 0.7202\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 1.2950 - acc: 0.5060 - val_loss: 0.6914 - val_acc: 0.7334\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 1.2886 - acc: 0.5104 - val_loss: 0.6870 - val_acc: 0.7243\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 1.2812 - acc: 0.5137 - val_loss: 0.6891 - val_acc: 0.7243\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 1.2858 - acc: 0.5104 - val_loss: 0.6703 - val_acc: 0.7302\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 1.2837 - acc: 0.5149 - val_loss: 0.6990 - val_acc: 0.7084\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 1.2743 - acc: 0.5214 - val_loss: 0.6832 - val_acc: 0.7221\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 1.2819 - acc: 0.5155 - val_loss: 0.6921 - val_acc: 0.7188\n",
      "Test accuracy: 0.7188\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 1.9837 - acc: 0.2851 - val_loss: 1.0115 - val_acc: 0.7049\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 16s 271us/step - loss: 1.3979 - acc: 0.4690 - val_loss: 0.6996 - val_acc: 0.7768\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 1.2618 - acc: 0.5126 - val_loss: 0.6287 - val_acc: 0.7804\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 1.1949 - acc: 0.5329 - val_loss: 0.6215 - val_acc: 0.7918\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 1.1642 - acc: 0.5453 - val_loss: 0.6011 - val_acc: 0.7590\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 1.1371 - acc: 0.5564 - val_loss: 0.5935 - val_acc: 0.7855\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 16s 264us/step - loss: 1.1227 - acc: 0.5624 - val_loss: 0.5634 - val_acc: 0.8089\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 1.0988 - acc: 0.5757 - val_loss: 0.5583 - val_acc: 0.8108\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 1.0850 - acc: 0.5844 - val_loss: 0.5350 - val_acc: 0.8115\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 1.0737 - acc: 0.5921 - val_loss: 0.5440 - val_acc: 0.8200\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 1.0571 - acc: 0.5976 - val_loss: 0.5491 - val_acc: 0.8225\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 1.0411 - acc: 0.6047 - val_loss: 0.5176 - val_acc: 0.8350\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 16s 272us/step - loss: 1.0362 - acc: 0.6113 - val_loss: 0.5227 - val_acc: 0.8320\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 16s 275us/step - loss: 1.0352 - acc: 0.6119 - val_loss: 0.5206 - val_acc: 0.8344\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 1.0239 - acc: 0.6196 - val_loss: 0.5232 - val_acc: 0.8329\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 1.0205 - acc: 0.6183 - val_loss: 0.5149 - val_acc: 0.8303\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 1.0162 - acc: 0.6227 - val_loss: 0.4939 - val_acc: 0.8486\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 1.0060 - acc: 0.6274 - val_loss: 0.4837 - val_acc: 0.8499\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 1.0019 - acc: 0.6297 - val_loss: 0.4913 - val_acc: 0.8437\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.9921 - acc: 0.6330 - val_loss: 0.5001 - val_acc: 0.8357\n",
      "Test accuracy: 0.8357\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.8710 - acc: 0.6797 - val_loss: 0.4683 - val_acc: 0.8274\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.5399 - acc: 0.8096 - val_loss: 0.4220 - val_acc: 0.8441\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.4759 - acc: 0.8326 - val_loss: 0.3778 - val_acc: 0.8619\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.4407 - acc: 0.8456 - val_loss: 0.3732 - val_acc: 0.8638\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.4146 - acc: 0.8532 - val_loss: 0.3570 - val_acc: 0.8701\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.3965 - acc: 0.8603 - val_loss: 0.3521 - val_acc: 0.8691\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.3764 - acc: 0.8668 - val_loss: 0.3447 - val_acc: 0.8689\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.3621 - acc: 0.8703 - val_loss: 0.3337 - val_acc: 0.8786\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.3481 - acc: 0.8750 - val_loss: 0.3278 - val_acc: 0.8776\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3404 - acc: 0.8785 - val_loss: 0.3229 - val_acc: 0.8837\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3278 - acc: 0.8832 - val_loss: 0.3089 - val_acc: 0.8855\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.3245 - acc: 0.8836 - val_loss: 0.3078 - val_acc: 0.8895\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.3151 - acc: 0.8870 - val_loss: 0.3095 - val_acc: 0.8891\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.3089 - acc: 0.8886 - val_loss: 0.3112 - val_acc: 0.8850\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3003 - acc: 0.8909 - val_loss: 0.3004 - val_acc: 0.8883\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2940 - acc: 0.8931 - val_loss: 0.3046 - val_acc: 0.8900\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.2877 - acc: 0.8949 - val_loss: 0.3088 - val_acc: 0.8905\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.2840 - acc: 0.8959 - val_loss: 0.3216 - val_acc: 0.8800\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.2765 - acc: 0.8996 - val_loss: 0.2953 - val_acc: 0.8898\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.2738 - acc: 0.8996 - val_loss: 0.3071 - val_acc: 0.8897\n",
      "Test accuracy: 0.8897\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 26s 437us/step - loss: 0.5515 - acc: 0.7955 - val_loss: 0.3971 - val_acc: 0.8538\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 25s 419us/step - loss: 0.3912 - acc: 0.8559 - val_loss: 0.3930 - val_acc: 0.8547\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 24s 408us/step - loss: 0.3594 - acc: 0.8697 - val_loss: 0.4059 - val_acc: 0.8573\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.3438 - acc: 0.8755 - val_loss: 0.3364 - val_acc: 0.8774\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 26s 426us/step - loss: 0.3342 - acc: 0.8798 - val_loss: 0.3621 - val_acc: 0.8655\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 26s 435us/step - loss: 0.3275 - acc: 0.8823 - val_loss: 0.3800 - val_acc: 0.8672\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 25s 410us/step - loss: 0.3226 - acc: 0.8835 - val_loss: 0.3909 - val_acc: 0.8736\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 25s 421us/step - loss: 0.3208 - acc: 0.8863 - val_loss: 0.3959 - val_acc: 0.8746\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 24s 402us/step - loss: 0.3170 - acc: 0.8889 - val_loss: 0.3877 - val_acc: 0.8699\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 26s 436us/step - loss: 0.3140 - acc: 0.8894 - val_loss: 0.4106 - val_acc: 0.8793\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 26s 425us/step - loss: 0.3167 - acc: 0.8884 - val_loss: 0.3500 - val_acc: 0.8821\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 25s 418us/step - loss: 0.3087 - acc: 0.8914 - val_loss: 0.3666 - val_acc: 0.8831\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 25s 418us/step - loss: 0.3126 - acc: 0.8920 - val_loss: 0.3688 - val_acc: 0.8790\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 24s 403us/step - loss: 0.3056 - acc: 0.8945 - val_loss: 0.3701 - val_acc: 0.8792\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 26s 440us/step - loss: 0.3011 - acc: 0.8963 - val_loss: 0.3898 - val_acc: 0.8714\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 25s 412us/step - loss: 0.2994 - acc: 0.8964 - val_loss: 0.3960 - val_acc: 0.8741\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 26s 428us/step - loss: 0.2949 - acc: 0.8972 - val_loss: 0.3483 - val_acc: 0.8843\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 26s 440us/step - loss: 0.2929 - acc: 0.8984 - val_loss: 0.3972 - val_acc: 0.8765\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 25s 419us/step - loss: 0.2886 - acc: 0.9005 - val_loss: 0.3596 - val_acc: 0.8804\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 26s 425us/step - loss: 0.2866 - acc: 0.9019 - val_loss: 0.3590 - val_acc: 0.8879\n",
      "Test accuracy: 0.8879\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 2.4394 - acc: 0.1056 - val_loss: 2.3028 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 2.3134 - acc: 0.1091 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 2.3028 - acc: 0.1139 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 2.2943 - acc: 0.1189 - val_loss: 2.2981 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 2.2816 - acc: 0.1279 - val_loss: 2.2744 - val_acc: 0.2025\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 2.2488 - acc: 0.1379 - val_loss: 2.1218 - val_acc: 0.2231\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 2.1809 - acc: 0.1571 - val_loss: 1.8866 - val_acc: 0.3261\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 2.0835 - acc: 0.1746 - val_loss: 1.7313 - val_acc: 0.3511\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 1.9830 - acc: 0.1882 - val_loss: 1.6722 - val_acc: 0.2107\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 1.9127 - acc: 0.1993 - val_loss: 1.6555 - val_acc: 0.2468\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 1.8632 - acc: 0.2090 - val_loss: 1.6532 - val_acc: 0.3076\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 1.8344 - acc: 0.2094 - val_loss: 1.6441 - val_acc: 0.2973\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 1.8099 - acc: 0.2155 - val_loss: 1.6417 - val_acc: 0.2351\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 1.7900 - acc: 0.2193 - val_loss: 1.6358 - val_acc: 0.2256\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 1.7733 - acc: 0.2256 - val_loss: 1.6272 - val_acc: 0.2542\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.7670 - acc: 0.2276 - val_loss: 1.6121 - val_acc: 0.2473\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 1.7535 - acc: 0.2341 - val_loss: 1.6026 - val_acc: 0.3279\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.7410 - acc: 0.2396 - val_loss: 1.5732 - val_acc: 0.3402\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 1.7296 - acc: 0.2518 - val_loss: 1.5431 - val_acc: 0.3465\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 1.7142 - acc: 0.2610 - val_loss: 1.5063 - val_acc: 0.3489\n",
      "Test accuracy: 0.3489\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 0.6747 - acc: 0.7636 - val_loss: 0.4946 - val_acc: 0.8305\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.5180 - acc: 0.8286 - val_loss: 0.4569 - val_acc: 0.8268\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.5110 - acc: 0.8374 - val_loss: 0.4346 - val_acc: 0.8586\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.5154 - acc: 0.8383 - val_loss: 0.4375 - val_acc: 0.8599\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 20s 333us/step - loss: 0.5271 - acc: 0.8393 - val_loss: 0.4438 - val_acc: 0.8567\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 21s 356us/step - loss: 0.5332 - acc: 0.8434 - val_loss: 0.4442 - val_acc: 0.8516\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 21s 354us/step - loss: 0.5305 - acc: 0.8425 - val_loss: 0.4934 - val_acc: 0.8491\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 0.5342 - acc: 0.8428 - val_loss: 0.4444 - val_acc: 0.8632\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 0.5451 - acc: 0.8407 - val_loss: 0.4520 - val_acc: 0.8622\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 20s 339us/step - loss: 0.5450 - acc: 0.8430 - val_loss: 0.4992 - val_acc: 0.8409\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.5638 - acc: 0.8448 - val_loss: 0.5148 - val_acc: 0.8507\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.5768 - acc: 0.8439 - val_loss: 0.4772 - val_acc: 0.8489\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.5841 - acc: 0.8438 - val_loss: 0.4838 - val_acc: 0.8688\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.5972 - acc: 0.8428 - val_loss: 0.5183 - val_acc: 0.8663\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.5873 - acc: 0.8449 - val_loss: 0.4613 - val_acc: 0.8649\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.6119 - acc: 0.8451 - val_loss: 0.5557 - val_acc: 0.8613\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.6044 - acc: 0.8429 - val_loss: 0.5408 - val_acc: 0.8615\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.6265 - acc: 0.8433 - val_loss: 0.5055 - val_acc: 0.8579\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.6146 - acc: 0.8425 - val_loss: 0.5083 - val_acc: 0.8578\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.6379 - acc: 0.8425 - val_loss: 0.5539 - val_acc: 0.8620\n",
      "Test accuracy: 0.862\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.9315 - acc: 0.2959 - val_loss: 1.2389 - val_acc: 0.6340\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 1.2197 - acc: 0.5397 - val_loss: 0.8891 - val_acc: 0.6995\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.9914 - acc: 0.6231 - val_loss: 0.7726 - val_acc: 0.7172\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.8836 - acc: 0.6624 - val_loss: 0.7155 - val_acc: 0.7406\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.8247 - acc: 0.6835 - val_loss: 0.6718 - val_acc: 0.7481\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.7857 - acc: 0.7002 - val_loss: 0.6500 - val_acc: 0.7503\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.7560 - acc: 0.7109 - val_loss: 0.6255 - val_acc: 0.7668\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.7307 - acc: 0.7223 - val_loss: 0.6082 - val_acc: 0.7731\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.7090 - acc: 0.7320 - val_loss: 0.5998 - val_acc: 0.7724\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.7001 - acc: 0.7361 - val_loss: 0.5919 - val_acc: 0.7811\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.6794 - acc: 0.7457 - val_loss: 0.5789 - val_acc: 0.7866\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.6695 - acc: 0.7493 - val_loss: 0.5707 - val_acc: 0.7854\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.6563 - acc: 0.7542 - val_loss: 0.5574 - val_acc: 0.7940\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.6492 - acc: 0.7588 - val_loss: 0.5457 - val_acc: 0.7988\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.6372 - acc: 0.7629 - val_loss: 0.5355 - val_acc: 0.8035\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.6298 - acc: 0.7679 - val_loss: 0.5285 - val_acc: 0.8074\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.6195 - acc: 0.7707 - val_loss: 0.5214 - val_acc: 0.8094\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.6107 - acc: 0.7744 - val_loss: 0.5132 - val_acc: 0.8185\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.6028 - acc: 0.7781 - val_loss: 0.5112 - val_acc: 0.8151\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.5959 - acc: 0.7801 - val_loss: 0.5010 - val_acc: 0.8197\n",
      "Test accuracy: 0.8197\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.6804 - acc: 0.7451 - val_loss: 0.4301 - val_acc: 0.8468\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.4816 - acc: 0.8246 - val_loss: 0.4076 - val_acc: 0.8498\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.4424 - acc: 0.8390 - val_loss: 0.3782 - val_acc: 0.8617\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.4186 - acc: 0.8473 - val_loss: 0.3605 - val_acc: 0.8696\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.4046 - acc: 0.8526 - val_loss: 0.3589 - val_acc: 0.8709\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.3925 - acc: 0.8586 - val_loss: 0.3489 - val_acc: 0.8749\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.3867 - acc: 0.8622 - val_loss: 0.3647 - val_acc: 0.8679\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.3777 - acc: 0.8637 - val_loss: 0.3397 - val_acc: 0.8776\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.3723 - acc: 0.8661 - val_loss: 0.3448 - val_acc: 0.8731\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.3666 - acc: 0.8686 - val_loss: 0.3425 - val_acc: 0.8775\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.3613 - acc: 0.8715 - val_loss: 0.3374 - val_acc: 0.8776\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.3575 - acc: 0.8723 - val_loss: 0.3331 - val_acc: 0.8808\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.3559 - acc: 0.8736 - val_loss: 0.3327 - val_acc: 0.8800\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.3518 - acc: 0.8748 - val_loss: 0.3465 - val_acc: 0.8778\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.3507 - acc: 0.8758 - val_loss: 0.3197 - val_acc: 0.8853\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.3467 - acc: 0.8763 - val_loss: 0.3253 - val_acc: 0.8805\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.3473 - acc: 0.8775 - val_loss: 0.3260 - val_acc: 0.8825\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.3447 - acc: 0.8788 - val_loss: 0.3241 - val_acc: 0.8852\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.3399 - acc: 0.8800 - val_loss: 0.3197 - val_acc: 0.8826\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.3422 - acc: 0.8795 - val_loss: 0.3215 - val_acc: 0.8863\n",
      "Test accuracy: 0.8863\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.5637 - acc: 0.7939 - val_loss: 0.4312 - val_acc: 0.8423\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 18s 306us/step - loss: 0.4156 - acc: 0.8471 - val_loss: 0.3654 - val_acc: 0.8624\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 18s 296us/step - loss: 0.3796 - acc: 0.8613 - val_loss: 0.3473 - val_acc: 0.8721\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.3547 - acc: 0.8684 - val_loss: 0.3324 - val_acc: 0.8732\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.3403 - acc: 0.8747 - val_loss: 0.3227 - val_acc: 0.8819\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.3265 - acc: 0.8793 - val_loss: 0.3151 - val_acc: 0.8809\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.3143 - acc: 0.8827 - val_loss: 0.3107 - val_acc: 0.8871\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3164 - val_acc: 0.8838\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 18s 295us/step - loss: 0.2971 - acc: 0.8891 - val_loss: 0.2970 - val_acc: 0.8884\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 18s 308us/step - loss: 0.2876 - acc: 0.8917 - val_loss: 0.2904 - val_acc: 0.8933\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 19s 313us/step - loss: 0.2829 - acc: 0.8938 - val_loss: 0.2912 - val_acc: 0.8955\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.2751 - acc: 0.8965 - val_loss: 0.2948 - val_acc: 0.8905\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.2717 - acc: 0.8965 - val_loss: 0.2847 - val_acc: 0.8982\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.2665 - acc: 0.8994 - val_loss: 0.2813 - val_acc: 0.8974\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 0.2586 - acc: 0.9022 - val_loss: 0.2812 - val_acc: 0.8971\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.2505 - acc: 0.9041 - val_loss: 0.2877 - val_acc: 0.8966\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.2493 - acc: 0.9052 - val_loss: 0.2865 - val_acc: 0.8971\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.2435 - acc: 0.9079 - val_loss: 0.2805 - val_acc: 0.8992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.2394 - acc: 0.9089 - val_loss: 0.2726 - val_acc: 0.8999\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.2344 - acc: 0.9106 - val_loss: 0.2692 - val_acc: 0.9018\n",
      "Test accuracy: 0.9018\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 2s 157us/step\n",
      "[0.269217800784111, 0.9018]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Activation': 1, 'Dense': 2, 'Dropout': 0.4143619965361732, 'Dropout_1': 0.09225974322037533, 'add': 1, 'batch_size': 1, 'conditional': 0, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=rand.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Keras optim')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4b1c770b8959>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trial' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
